{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import os\n",
    "\n",
    "tokenizer_folder = \"TokRoBERTa_BPE\"\n",
    "if os.path.isdir(tokenizer_folder) == False:\n",
    "    os.mkdir(tokenizer_folder)\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=[\"hi_dedup_1000.txt\"], vocab_size=32000, min_frequency=3, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TokRoBERTa_BPE/vocab.json', 'TokRoBERTa_BPE/merges.txt']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model(tokenizer_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./TokRoBERTa_BPE/vocab.json\",\n",
    "    \"./TokRoBERTa_BPE/merges.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=41, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"महाप्रबंधक भारत संचार निगम लिमिटेड दुर्ग को सम्पत्तिकर\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'à¤®à¤¹',\n",
       " 'à¤¾',\n",
       " 'à¤ª',\n",
       " 'à¥į',\n",
       " 'à¤°à¤¬',\n",
       " 'à¤Ĥ',\n",
       " 'à¤§à¤ķ',\n",
       " 'Ġà¤Ń',\n",
       " 'à¤¾',\n",
       " 'à¤°à¤¤',\n",
       " 'Ġà¤¸',\n",
       " 'à¤Ĥ',\n",
       " 'à¤ļ',\n",
       " 'à¤¾',\n",
       " 'à¤°',\n",
       " 'Ġà¤¨',\n",
       " 'à¤¿',\n",
       " 'à¤Ĺà¤®',\n",
       " 'Ġà¤²',\n",
       " 'à¤¿',\n",
       " 'à¤®',\n",
       " 'à¤¿',\n",
       " 'à¤Ł',\n",
       " 'à¥ĩ',\n",
       " 'à¤¡',\n",
       " 'Ġà¤¦',\n",
       " 'à¥ģ',\n",
       " 'à¤°',\n",
       " 'à¥į',\n",
       " 'à¤Ĺ',\n",
       " 'Ġà¤ķ',\n",
       " 'à¥ĭ',\n",
       " 'Ġà¤¸à¤®',\n",
       " 'à¥į',\n",
       " 'à¤ªà¤¤',\n",
       " 'à¥į',\n",
       " 'à¤¤',\n",
       " 'à¤¿',\n",
       " 'à¤ķà¤°',\n",
       " '</s>']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"महाप्रबंधक भारत संचार निगम लिमिटेड दुर्ग को सम्पत्तिकर\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 8\n",
    "TRAIN_EPOCHS = 15\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0.01\n",
    "SEED = 42\n",
    "MAX_LEN = 128\n",
    "SUMMARY_LEN = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    # vocab_size=32000,\n",
    "    # max_position_embeddings=514,\n",
    "    # num_attention_heads=12,\n",
    "    # num_hidden_layers=6,\n",
    "    # type_vocab_size=1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    bos_token_id=0,\n",
    "    eos_token_id=2,\n",
    "    gradient_checkpointing=False,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    hidden_size=768,\n",
    "    initializer_range=0.02,\n",
    "    intermediate_size=3072,\n",
    "    layer_norm_eps=1e-05,\n",
    "    max_position_embeddings=514,\n",
    "    model_type=\"roberta\",\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=12,\n",
    "    pad_token_id=1,\n",
    "    position_embedding_type=\"absolute\",\n",
    "    transformers_version=\"4.4.0.dev0\",\n",
    "    type_vocab_size=1,\n",
    "    use_cache=True,\n",
    "    vocab_size=32000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./TokRoBERTa_BPE\", max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110651648"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dishantpadalia/opt/miniconda3/envs/trocr/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./hi_dedup_1000.txt\",\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./TokRoBERTa_BPE\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_gpu_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "/Users/dishantpadalia/opt/miniconda3/envs/trocr/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 16\n",
      "  Number of trainable parameters = 110651648\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c316d2e9594c79a0f474b4b23af1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 208.3559, 'train_samples_per_second': 4.799, 'train_steps_per_second': 0.077, 'train_loss': 8.832284927368164, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16, training_loss=8.832284927368164, metrics={'train_runtime': 208.3559, 'train_samples_per_second': 4.799, 'train_steps_per_second': 0.077, 'train_loss': 8.832284927368164, 'epoch': 1.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./TokRoBERTa_BPE/\n",
      "Configuration saved in ./TokRoBERTa_BPE/config.json\n",
      "Model weights saved in ./TokRoBERTa_BPE/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./TokRoBERTa_BPE/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./TokRoBERTa_BPE/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./TokRoBERTa_BPE/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file ./TokRoBERTa_BPE/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./TokRoBERTa_BPE/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file ./TokRoBERTa_BPE/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at ./TokRoBERTa_BPE/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file ./TokRoBERTa_BPE/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./TokRoBERTa_BPE/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file ./TokRoBERTa_BPE/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./TokRoBERTa_BPE/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file ./TokRoBERTa_BPE/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./TokRoBERTa_BPE/\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./TokRoBERTa_BPE/\",\n",
    "    tokenizer=\"./TokRoBERTa_BPE/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.017935452982783318,\n",
       "  'token': 264,\n",
       "  'token_str': 'ा',\n",
       "  'sequence': 'रहा है वह न सिर्फ चिंताजनका'},\n",
       " {'score': 0.005482870154082775,\n",
       "  'token': 267,\n",
       "  'token_str': '्',\n",
       "  'sequence': 'रहा है वह न सिर्फ चिंताजनक्'},\n",
       " {'score': 0.004985986743122339,\n",
       "  'token': 265,\n",
       "  'token_str': 'े',\n",
       "  'sequence': 'रहा है वह न सिर्फ चिंताजनके'},\n",
       " {'score': 0.002476259833201766,\n",
       "  'token': 270,\n",
       "  'token_str': 'ि',\n",
       "  'sequence': 'रहा है वह न सिर्फ चिंताजनकि'},\n",
       " {'score': 0.0016431399853900075,\n",
       "  'token': 269,\n",
       "  'token_str': 'ी',\n",
       "  'sequence': 'रहा है वह न सिर्फ चिंताजनकी'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"रहा है वह न सिर्फ चिंताजनक <mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.01747768558561802,\n",
       "  'token': 264,\n",
       "  'token_str': 'ा',\n",
       "  'sequence': 'पहलू सामने आा । अक्सर कंपनियों'},\n",
       " {'score': 0.005214308854192495,\n",
       "  'token': 267,\n",
       "  'token_str': '्',\n",
       "  'sequence': 'पहलू सामने आ् । अक्सर कंपनियों'},\n",
       " {'score': 0.005012022331357002,\n",
       "  'token': 265,\n",
       "  'token_str': 'े',\n",
       "  'sequence': 'पहलू सामने आे । अक्सर कंपनियों'},\n",
       " {'score': 0.0025437825825065374,\n",
       "  'token': 270,\n",
       "  'token_str': 'ि',\n",
       "  'sequence': 'पहलू सामने आि । अक्सर कंपनियों'},\n",
       " {'score': 0.0017022050451487303,\n",
       "  'token': 269,\n",
       "  'token_str': 'ी',\n",
       "  'sequence': 'पहलू सामने आी । अक्सर कंपनियों'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"पहलू सामने आ <mask> । अक्सर कंपनियों\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.018545446917414665,\n",
       "  'token': 264,\n",
       "  'token_str': 'ा',\n",
       "  'sequence': 'मैं उनसे नफरत करती हूँ जिन्होंने किसी को अच्छे काम करने से रोका है. मैं नहीं जानती कि इसमें कितना समय और लगेगा, और मैं यह भी नहीं जानती कि मेरी मां घर वापस कब आएँगी, मुझे डाटेंगी, मुझे खाना खाने को कहेंगी, या मुझेा'},\n",
       " {'score': 0.00518343411386013,\n",
       "  'token': 267,\n",
       "  'token_str': '्',\n",
       "  'sequence': 'मैं उनसे नफरत करती हूँ जिन्होंने किसी को अच्छे काम करने से रोका है. मैं नहीं जानती कि इसमें कितना समय और लगेगा, और मैं यह भी नहीं जानती कि मेरी मां घर वापस कब आएँगी, मुझे डाटेंगी, मुझे खाना खाने को कहेंगी, या मुझे्'},\n",
       " {'score': 0.004980894271284342,\n",
       "  'token': 265,\n",
       "  'token_str': 'े',\n",
       "  'sequence': 'मैं उनसे नफरत करती हूँ जिन्होंने किसी को अच्छे काम करने से रोका है. मैं नहीं जानती कि इसमें कितना समय और लगेगा, और मैं यह भी नहीं जानती कि मेरी मां घर वापस कब आएँगी, मुझे डाटेंगी, मुझे खाना खाने को कहेंगी, या मुझेे'},\n",
       " {'score': 0.0024795588105916977,\n",
       "  'token': 270,\n",
       "  'token_str': 'ि',\n",
       "  'sequence': 'मैं उनसे नफरत करती हूँ जिन्होंने किसी को अच्छे काम करने से रोका है. मैं नहीं जानती कि इसमें कितना समय और लगेगा, और मैं यह भी नहीं जानती कि मेरी मां घर वापस कब आएँगी, मुझे डाटेंगी, मुझे खाना खाने को कहेंगी, या मुझेि'},\n",
       " {'score': 0.001678893924690783,\n",
       "  'token': 269,\n",
       "  'token_str': 'ी',\n",
       "  'sequence': 'मैं उनसे नफरत करती हूँ जिन्होंने किसी को अच्छे काम करने से रोका है. मैं नहीं जानती कि इसमें कितना समय और लगेगा, और मैं यह भी नहीं जानती कि मेरी मां घर वापस कब आएँगी, मुझे डाटेंगी, मुझे खाना खाने को कहेंगी, या मुझेी'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"मैं उनसे नफरत करती हूँ जिन्होंने किसी को अच्छे काम करने से रोका है. मैं नहीं जानती कि इसमें कितना समय और लगेगा, और मैं यह भी नहीं जानती कि मेरी मां घर वापस कब आएँगी, मुझे डाटेंगी, मुझे खाना खाने को कहेंगी, या मुझे <mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: transformers-cli <command> [<args>]\n",
      "Transformers CLI tool: error: unrecognized arguments: ./TokRoBERTa_BPE\n"
     ]
    }
   ],
   "source": [
    "!transformers-cli add-new-model ./TokRoBERTa_BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f416df2e98a2debd3c815f38591d1eaeb62c97d7a01f9e221ecccaca550bc21b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
